---
title: PyTorch 学习
description: >-
  PyTorch basics, will evolve into intermediary, and finally advanced)
author: cybotiger
date: 2025-04-22 12:00:00 +0800
categories: [编程语言, 深度学习框架]
tags: []
math: true
mermaid: true
---

## Autograd
Autograd is a reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.

Internally, autograd represents this graph as a graph of Function objects (really expressions), which can be apply() ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forward pass is completed, we evaluate this graph in the backwards pass to compute the gradients.

An important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.
### Computation Graph

#### 两类tensor/node
+ leaf node

    即神经网络中的 weight，`nn.Module` 中的参数张量
    
    这些 node 没有 `grad_fn`，设置 `requires_grad=True` 时，调用 non-leaf node 的 `.bakcward()` 会积累梯度

+ non-leaf node

    即神经网络中的 activation，每一个 node 都捆绑着计算图中一个 operation
    
    这些 node 有 `grad_fn`，调用 `.bakcward()` 时会积累梯度，在 `optimizer.step()` 时可能会用到它们的梯度。它们的梯度作为计算的中间结果，一般是不可见的。

> 冻结权重参数张量可直接调用 `.requires_grad_(False)`
{: .prompt-tip }

### Grad Mode
+ Default Mode(grad Mode)

    默认模式，对每个操作都会记录到计算图中。`requires_grad=True` 仅在默认模式下有效

+ no-grad Mode

    操作不记录到计算图。存储计算结果，并记录梯度

+ Inference Mode

    计算结果也不记录

+ Evaluation Mode (`nn.Module.eval()`)

    和上述 3 种 mode 无关。建议 evaluate 时用 `model.eval()`，train 时用 `model.train()`