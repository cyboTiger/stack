---
title: PPO算法
description: >-
  RL基础
author: cybotiger
date: 2025-03-17 12:00:00 +0800
categories: [AI]
tags: []
pin: false
math: true
mermaid: true
---

## 1. 背景
策略梯度优化中最常用的 gradient estimator 是

$$
\hat{g}=\hat{\mathbb{E}}_{t}\Big{[}\nabla_{\theta}\log\pi_{\theta}(a_{t}\,|\,s_{t})\hat{A}_{t}\Big{]}
$$

因此可以构造最原始的 objective function 如下：

$$
L^{PG}(\theta)=\hat{\mathbb{E}}_{t}\Big{[}\log\pi_{\theta}(a_{t}\,|\,s_{t})\hat{A}_{t}\Big{]}
$$

### TRPO算法
全称 Trust Region Policy Optimization，为了避免更新后的策略偏离原来策略过多，引入 KL 散度限制，在此限制下最大化 objective function 如下：

$$
\max_\theta\ \ \ 
\hat{\mathbb{E}}_{t}\Big{[}\frac{\pi_{\theta}(a_{t}\,|\,s_{t})}{\pi_{\theta_{old}}(a_{t}\,|\,s_{t})}\hat{A}_{t}\Big{]}
$$

$$
\text{subject to: }\ \ \ 
\hat{\mathbb{E}}_{t}[\text{KL}[\pi_{\theta_{old}}(\cdot\,|\,s_{t}),\pi_{\theta}(\cdot\,|\,s_{t})]] \leq \delta
$$

实际中，使用共轭梯度方法求解该问题，对 objective 使用一阶近似，对 constraint 使用二阶近似；

TRPO 算法的理论依据是将上述的限制转化为惩罚项，即优化以下目标：

$$
\hat{\mathbb{E}}_{t}\Big{[}\frac{\pi_{\theta}(a_{t}\,|\,s_{t})}{\pi_{\theta_{old}}(a_{t}\,|\,s_{t})}\hat{A}_{t}-\beta\,\text{KL}[\pi_{\theta_{old}}(\cdot\,|\,s_{t}),\pi_{\theta}(\cdot\,|\,s_{t})]\Big{]}
$$

但在实际过程中，很难确定惩罚项系数 $\beta$ 的值，一般需要在学习的过程中动态的调整，即 adaptive KL penalty

## 2. PPO算法
记概率比 $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$（于是有 $r_t(\theta_{\text{old}}) = 1$）。TRPO 的目标函数为：

$$
L^{CPI}(\theta) = \hat{E}_t \left[ \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} \hat{A}_t \right] = \hat{E}_t \left[ r_t(\theta) \hat{A}_t \right]
$$

而 PPO 进行了裁剪(clip)，来防止策略偏离过大：

$$
L^{CLIP}(\theta) = \hat{E}_t \left[ \min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right) \right]
$$

可以看到 $L^{CLIP}(\theta)$ 是 $L^{PG}(\theta)$ 的下界，只会更差，不会更好

![PPO objective](/assets/img/rl/image.png)

### 具体算法
#### 优势函数
多数优势函数会用到一个可学习的状态-价值函数 $V(s)$ 来估计当前状态下的价值，比如 GAE(Generalized Advantage Estimator)。PPO 使用的是 GAE 的截断版本（从 $\infty$ 截断到 $T$ 时间步），在 rollout 时进行 T 步采样，计算每一步 t 的优势：

$$
\hat{A}_{t} = \delta_{t} + (\gamma\lambda)\delta_{t+1} + \cdots + (\gamma\lambda)^{T-t-1}\delta_{T-1}
$$

$$
\text{where}\ \ \delta_{t} = r_{t} + \gamma V(s_{t+1}) - V(s_{t})
$$

> 当 $\lambda=1$ 时，该式退化为 
> 
> $$ 
> \hat{A}_{t} = -V(s_{t}) + r_{t} + \gamma r_{t+1} + \cdots + \gamma^{T-t-1}r_{T-1} + \gamma^{T-t}V(s_{T}) 
> $$ 
> 
> 即多步时序差分

PPO 算法的伪代码如下：

![ppo algo](/assets/img/rl/image1.png)

N 个 actor 并行采样生成 T 步轨迹，利用这 NT 个样本，进行 K 个 epoch 的批次优化

#### 加强目标函数
如果 actor 和 critic（即 value model）共享参数，那么需要同时优化 critic 与目标价值的 loss；此外还可以引入 entropy bonus 来鼓励探索。综合上述，就有如下的加强版 PPO：

$$
L_{t}^{CLIP+VF+S}(\theta) = \hat{\mathbb{E}}_{t}\left[L_{t}^{CLIP}(\theta) - c_{1}L_{t}^{VF}(\theta) + c_{2}S[\pi_{\theta}](s_{t})\right]
$$


---

## 参考资料

[知乎：从策略梯度开始理解PPO算法](https://zhuanlan.zhihu.com/p/614115887)

### 1. 传统策略梯度算法

#### **1.1 从价值近似到策略近似**

强化学习算法可以分为两大类：基于值函数的强化学习和基于策略的强化学习。

- **基于值函数的强化学习**通过递归地求解贝尔曼方程来维护Q值函数（可以是离散的列表，也可以是神经网络），每次选择动作时会选择该状态下对应Q值最大的动作，使得未来积累的期望奖励值最大。这些算法在学习后的Q值函数不再发生变化，每次做出的策略也是一定的，可以理解为确定性策略。policy 如下：
    
    $$
    \pi:s \rightarrow a
    $$
    
- **基于策略的强化学习**不再通过价值函数来确定选择动作的策略，而是直接学习策略本身，通过一组参数 $\theta$ 对策略进行参数化，并通过神经网络方法优化。policy 如下：
    
    $$
    \pi_\theta(a|s)=P(a|s;\theta)
    $$
    

#### **1.2 定义目标函数**

$$
\max_{\theta} J(\theta) = \max_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta}} R(\tau) = \max_{\theta} \sum_{\tau} P(\tau;\theta)R(\tau)
$$

其中轨迹 $\tau$ 是agent与环境交互产生的状态-动作轨迹 $(s_1,a_1,s_2,a_2,...)$，服从 $\pi_\theta$ 的概率分布



### [AUX] 术语介绍
> Generated by Deepseek
{: .prompt-info }

在强化学习中，**优势函数（A）**、**价值函数（V）** 和 **奖励（R）** 是核心概念，它们共同描述了智能体在环境中的决策依据和长期收益的评估。以下是它们的定义、区别和关系：

---

#### 1. **奖励（R, Reward）**
   - **定义**：  
     奖励是环境在智能体执行某个动作后给出的**即时反馈信号**（标量值），表示该动作的短期好坏。  
     - 例如：游戏中得分增加（+1）、碰撞障碍物（-10）。
   - **数学表示**：  
     $ R_t $ 表示时间步 $ t $ 的即时奖励。
   - **特点**：  
     - 完全由环境决定，是强化学习中最基础的信号。
     - 仅反映当前动作的瞬时效果，不包含长期信息。

---

#### 2. **状态价值函数（V, State Value Function）**
   - **定义**：  
     状态价值函数 $ V^\pi(s) $ 表示在状态 $ s $ 下，**遵循策略 $ \pi $ 后能获得的预期累积奖励**（考虑未来奖励的折扣）。  
     - 回答：“当前状态 $ s $ 长期来看有多好？”
   - **数学表示**：  
     $$
     V^\pi(s) = \mathbb{E}_{\pi} \left[ \sum_{k=0}^\infty \gamma^k R_{t+k} \mid S_t = s \right]
     $$
     - $ \gamma \in [0,1] $ 是折扣因子，平衡即时奖励和未来奖励的重要性。
   - **特点**：  
     - 评价的是**状态**的长期价值，与具体动作无关。
     - 由策略 $ \pi $ 决定（不同策略下同一状态的价值可能不同）。

---

#### 3. **优势函数（A, Advantage Function）**
   - **定义**：  
     优势函数 $ A^\pi(s,a) $ 表示在状态 $ s $ 下执行动作 $ a $ **相比策略 $ \pi $ 的平均水平有多好**。  
     - 回答：“在状态 $ s $ 下选择动作 $ a $ 比默认策略好多少？”
   - **数学表示**：  
     $$
     A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
     $$
     其中 $ Q^\pi(s,a) $ 是动作价值函数（即状态-动作对的预期累积奖励）。
   - **特点**：  
     - 若 $ A > 0 $，说明动作 $ a $ 优于策略 $ \pi $ 的平均水平；反之则劣于平均水平。
     - 用于减少策略梯度的方差（如Actor-Critic方法）。

---

#### 三者的关系
##### 1. **数学关系**  
   - **动作价值函数 $ Q^\pi(s,a) $** 是核心桥梁：  
     
     $$
     Q^\pi(s,a) = \mathbb{E}_{\pi} \left[ R_t + \gamma V^\pi(S_{t+1}) \mid S_t=s, A_t=a \right]
     $$

   - **优势函数 $ A $** 通过 $ Q $ 和 $ V $ 计算：  
     
     $$
     A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
     $$

   - **价值函数 $ V $** 是 $ Q $ 对动作的期望：  
     
     $$
     V^\pi(s) = \mathbb{E}_{a \sim \pi} \left[ Q^\pi(s,a) \right]
     $$

##### 2. **直观理解**  
   - **奖励 $ R $** 是即时反馈，**价值 $ V $** 是长期评价，**优势 $ A $** 是动作的相对优势。  
   - **例子**（游戏场景）：  
     - 当前状态 $ s $：玩家面对敌人。  
     - 动作 $ a $：发射子弹。  
     - $ R $：击中敌人（+10），未击中（0）。  
     - $ V(s) $：当前状态的长期预期得分（如+50）。  
     - $ Q(s,a) $：选择“发射子弹”的长期预期得分（如+60）。  
     - $ A(s,a) = Q(s,a) - V(s) = +10 $ → 发射子弹比平均策略更好。

---

#### 关键区别

| 概念           | 评价对象      | 依赖关系         | 用途                  |
|:--------------|:-------------|:----------------|:-----------------------|
| **奖励 $ R $** | 即时动作的效果 | 由环境直接给出    | 策略优化的原始信号        |
| **价值 $ V $** | 状态的长期价值 | 依赖策略 $ \pi $ | 评估状态好坏，作为基线     |
| **优势 $ A $** | 动作的相对优势 | $ A = Q - V $   | 减少方差，指导策略更新方向 |

---

#### 在PPO中的具体应用
1. **计算优势函数**：  
   PPO 使用广义优势估计（GAE）来平衡偏差和方差：  
   
   $$
   A_t^{\text{GAE}} = \sum_{k=0}^{T-t} (\gamma \lambda)^k \delta_{t+k}, \quad \delta_t = R_t + \gamma V(S_{t+1}) - V(S_t)
   $$

   - $ \lambda $ 是GAE的超参数，控制方差与偏差的权衡。

2. **策略更新**：  
   PPO 的损失函数利用优势函数 $ A $ 加权策略更新：  
   
   $$
   L^{\text{CLIP}} = -\mathbb{E} \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
   $$

   - $ r_t(\theta) $ 是新旧策略的重要性采样比率。

---

#### 总结
- **奖励 $ R $** 是环境的即时反馈，**价值 $ V $** 是状态的长期评价，**优势 $ A $** 是动作的相对改进潜力。  
- **关系链**：  
  
  $$
  R \rightarrow Q \rightarrow \begin{cases}
  V = \mathbb{E}_\pi [Q] \\
  A = Q - V
  \end{cases}
  $$

- 优势函数在策略梯度方法中至关重要，它帮助智能体识别哪些动作值得更多探索或利用。

在强化学习中，**重要性采样比率（Importance Sampling Ratio）** $ r_t(\theta) $ 和 PPO 的**截断（Clipping）**机制是保证策略梯度方法稳定性的核心设计。下面详细解释它们的作用和原理。

---

#### **1. 重要性采样比率 $ r_t(\theta) $ 的作用**
##### **（1）定义**
重要性采样比率衡量**新策略**和**旧策略**在相同状态下选择某个动作的概率比：
$$
r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
$$
其中：
- $ \pi_\theta $ 是**新策略**（待优化的策略）。
- $ \pi_{\theta_{\text{old}}} $ 是**旧策略**（采样数据时的策略）。

##### **（2）作用**
1. **修正策略更新的偏差**  
   - PPO 是一种**异策略（Off-Policy）**算法，即用于采样的策略（旧策略）和待优化的策略（新策略）不同。  
   - 由于新策略的参数 $ \theta $ 不断变化，直接用新策略的梯度更新会导致偏差。  
   - **重要性采样比率** $ r_t(\theta) $ 用于调整梯度更新，使其仍然适用于旧策略采样的数据。

2. **计算策略更新的权重**  
   - 在策略梯度法中，我们希望最大化 **优势函数 $ A_t $** 加权后的策略概率：
     
     $$
     \mathbb{E}_{a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} A_t \right]
     $$
     
   - 如果 $ r_t(\theta) > 1 $，说明新策略比旧策略更倾向于选择该动作，应该加强该动作的概率。  
   - 如果 $ r_t(\theta) < 1 $，说明新策略不太愿意选择该动作，应该降低其概率。

---

#### **2. 为什么 PPO 的损失函数要截断（Clipping）？**
##### **（1）问题：重要性采样比率的不稳定性**
- 如果新策略 $ \pi_\theta $ 和旧策略 $ \pi_{\theta_{\text{old}}} $ 差异太大，$ r_t(\theta) $ 可能会变得**极大或极小**，导致：
  1. **梯度爆炸**：如果 $ r_t(\theta) $ 极大，梯度更新可能过大，导致训练不稳定。
  2. **策略崩溃**：如果 $ r_t(\theta) $ 极小，某些动作的概率可能被过度压制，导致探索不足。

##### **（2）PPO 的解决方案：Clipped Surrogate Objective**
PPO 的损失函数通过**截断（Clipping）**限制 $ r_t(\theta) $ 的变化范围，确保策略更新**平滑且稳定**：
$$
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$
其中：
- $ \epsilon $ 是一个超参数（通常取 0.1~0.3），控制截断范围。
- $ \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) $ 将 $ r_t(\theta) $ 限制在 $ [1-\epsilon, 1+\epsilon] $ 之间。

##### **（3）截断的作用**
1. **防止策略更新过大**  
   - 如果 $ A_t > 0 $（动作比平均水平好），但 $ r_t(\theta) > 1+\epsilon $，则梯度更新被限制在 $ 1+\epsilon $，避免过度优化。  
   - 如果 $ A_t < 0 $（动作比平均水平差），但 $ r_t(\theta) < 1-\epsilon $，则梯度更新被限制在 $ 1-\epsilon $，避免过度惩罚。

2. **保证训练稳定性**  
   - 通过限制 $ r_t(\theta) $ 的变化范围，PPO 避免了传统策略梯度方法（如TRPO）中复杂的约束优化问题，同时仍能保证策略更新不会剧烈变化。

---

#### **3. 直观理解**
##### **（1）情况1：$ A_t > 0 $（好动作）**
- **未截断**：如果 $ r_t(\theta) $ 很大（比如 5），新策略会大幅提高该动作的概率，可能导致过度优化。  
- **截断后**：限制 $ r_t(\theta) \leq 1+\epsilon $，避免策略突变。

##### **（2）情况2：$ A_t < 0 $（坏动作）**
- **未截断**：如果 $ r_t(\theta) $ 很小（比如 0.1），新策略会大幅降低该动作的概率，可能导致探索不足。  
- **截断后**：限制 $ r_t(\theta) \geq 1-\epsilon $，避免策略过早放弃某些动作。

---

#### **4. 总结**

| **概念**                         | **作用**                                                                 | **PPO 的处理方式**                     |
|:--------------------------------|:------------------------------------------------------------------------|:----------|:------------------------|
| **重要性采样比率 $ r_t(\theta) $** | 修正新旧策略差异，计算梯度更新的权重。                                   | 通过重要性采样调整策略梯度。           |
| **截断（Clipping）**             | 防止 $ r_t(\theta) $ 过大或过小，避免梯度爆炸或策略崩溃。               | 限制 $ r_t(\theta) \in [1-\epsilon, 1+\epsilon] $。 |
| **优势函数 $ A_t $**   | 衡量动作的相对优势，决定更新方向（加强 or 削弱）。                        | 用于加权策略更新。                     |

PPO 通过**重要性采样比率**修正策略梯度，再通过**截断**机制保证训练稳定性，使其成为目前最流行的强化学习算法之一。

### [MISC] 个人理解
价值函数的理论计算似乎会无限的往后延伸。因为 t 时刻的价值 V 依赖于该时刻的 Q，而 Q 则依赖于 R 和下一时刻的 V，这样就会形成循环了。

因此，在强化学习（如PPO、RLHF等）中，**Critic模型（价值函数网络）的更新流程**采用近似估计。具体而言，通过最小化预测价值和目标价值的差异来更新Critic模型的参数。近似如下：


---

#### **1. Critic的理论优化目标**
Critic的任务是学习**状态价值函数 $ V^\pi(s) $**，即预测从状态 $ s $ 开始，遵循当前策略 $ \pi $ 能获得的**预期累积奖励**：
$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k} \mid S_t = s \right]
$$

---

#### **2. Critic更新的输入数据**
Critic的更新依赖于从环境中采样（或Actor生成）的轨迹数据，包括：
- 状态序列 $ s_t $
- 奖励序列 $ R_t $
- 下一状态 $ s_{t+1} $
- 终止标志 $ done_t $（可选）

---

#### **3. Critic更新的具体流程**
##### **步骤1：计算目标值（Target Value）**
Critic的目标是让 $ V(s_t) $ 逼近真实的累积奖励。目标值的计算方式有两种：
- **蒙特卡洛（MC）目标**（适用于完整轨迹）：
  $$
  V_{\text{target}}(s_t) = \sum_{k=0}^{T-t} \gamma^k R_{t+k}
  $$
- **时序差分（TD）目标**（更常用）：
  $$
  V_{\text{target}}(s_t) = R_t + \gamma (1 - done_t) V_{\text{old}}(s_{t+1})
  $$
  其中：
  - $ V_{\text{old}}(s_{t+1}) $ 是**旧Critic网络**（或当前Critic在更新前的输出）对下一状态的估计。
  - $ done_t $ 为1时表示终止状态（无未来奖励）。

##### **步骤2：计算损失函数**
Critic的更新通过最小化预测值 $ V(s_t) $ 与目标值 $ V_{\text{target}}(s_t) $ 的误差：
$$
L_{\text{Critic}} = \mathbb{E}_t \left[ \left( V(s_t) - V_{\text{target}}(s_t) \right)^2 \right]
$$
实际实现中通常使用**均方误差（MSE）**或**Huber损失**。

##### **步骤3：梯度下降更新**
使用优化器（如Adam）对Critic的参数 $ \phi $ 进行梯度下降：
$$
\phi \leftarrow \phi - \alpha \nabla_\phi L_{\text{Critic}}
$$
其中 $ \alpha $ 是学习率。

---

#### **4. 关键实现细节**
##### **(1) 目标值的稳定性**
- **目标网络（Target Network）**：  
  类似DQN，可以维护一个旧Critic网络（参数定期更新）来计算 $ V_{\text{target}} $，减少训练波动。
- **GAE（广义优势估计）**：  
  若使用GAE计算优势函数，Critic的目标值会基于多步TD误差调整（平衡偏差和方差）。

##### **(2) 数据来源**
- **同策略（On-Policy）**：  
  在PPO中，Critic和Actor使用**同一批最新数据**更新（数据在每次迭代后丢弃）。
- **异策略（Off-Policy）**：  
  某些算法（如SAC）可复用历史数据，但需重要性采样修正。

##### **(3) 与Actor的协同训练**
- **交替更新**：  
  通常先更新Critic多次（如10次），再更新Actor，确保价值估计足够准确。
- **共享网络结构**：  
  在部分实现中，Actor和Critic共享底层特征提取层（如Transformer的编码器），但输出层独立。

---

#### **5. 伪代码实现（PyTorch示例）**
```python
import torch
import torch.optim as optim

# 定义Critic网络
class Critic(torch.nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.fc = torch.nn.Sequential(
            torch.nn.Linear(state_dim, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 1)
        )

# 初始化
critic = Critic(state_dim)
optimizer = optim.Adam(critic.parameters(), lr=1e-3)

# 更新函数
def update_critic(states, rewards, next_states, dones, gamma=0.99):
    # 计算目标值
    with torch.no_grad():
        V_next = critic(next_states)
        V_target = rewards + gamma * (1 - dones) * V_next

    # 计算当前值
    V_current = critic(states)

    # 计算MSE损失
    loss = torch.nn.functional.mse_loss(V_current, V_target)

    # 梯度下降
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return loss.item()
```

---

#### **6. 在RLHF中的特殊考虑**
当Critic用于大语言模型（LLM）的RLHF训练时：
1. **输入状态**：  
   Critic的输入是文本序列（如生成的回答），需通过编码器（如Transformer）提取特征。
2. **奖励归一化**：  
   Reward Model的输出可能需归一化后作为Critic的训练目标。
3. **KL散度约束**：  
   可能需在Critic的损失中加入与Reference Model的KL散度项，防止过度偏离初始策略。

---

#### **7. 总结**

| **步骤**               | **操作**                                                                 |
|------------------------|--------------------------------------------------------------------------|
| 1. **数据收集**        | 通过Actor生成轨迹（状态、动作、奖励、下一状态）。                         |
| 2. **计算目标值**      | 使用MC或TD方法（如 $ V_{\text{target}} = r + \gamma V_{\text{old}}(s') $。 |
| 3. **计算损失**        | 最小化 $ \|V(s) - V_{\text{target}}\|^2 $。                            |
| 4. **梯度更新**        | 反向传播优化Critic参数。                                                 |

Critic的高效更新是稳定训练的关键，尤其在PPO中，准确的价值估计能显著提升策略优化的效果。